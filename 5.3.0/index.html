<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>llama_ros: llama.cpp for ROS 2: llama_ros</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">llama_ros: llama.cpp for ROS 2
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="doc-content">
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title"><a class="el" href="namespacellama__ros.html">llama_ros</a> </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a></p>
<p>This repository provides a set of ROS 2 packages to integrate <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> into ROS 2. Using the <a class="el" href="namespacellama__ros.html">llama_ros</a> packages, you can easily incorporate the powerful optimization capabilities of <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> into your ROS 2 projects by running <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">GGUF</a>-based <a href="https://huggingface.co/models?sort=trending&amp;search=gguf+7b">LLMs</a> and <a href="https://huggingface.co/models?sort=trending&amp;search=gguf+llava">VLMs</a>. You can also use features from <a class="el" href="llama_8cpp.html">llama.cpp</a> such as <a href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">GBNF grammars</a> and modify LoRAs in real-time.</p>
<div align="center"></div><div align="center"><a href="https://opensource.org/license/mit"><img src="https://img.shields.io/badge/GitHub-MIT-informational" alt="License: MIT" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/releases"><img src="https://img.shields.io/github/release/mgonzs13/llama_ros.svg" alt="GitHub release" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros?branch=main"><img src="https://img.shields.io/github/languages/code-size/mgonzs13/llama_ros.svg?branch=main" alt="Code Size" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/commits/main"><img src="https://img.shields.io/github/last-commit/mgonzs13/llama_ros.svg" alt="Last Commit" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/issues"><img src="https://img.shields.io/github/issues/mgonzs13/llama_ros" alt="GitHub issues" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/pulls"><img src="https://img.shields.io/github/issues-pr/mgonzs13/llama_ros" alt="GitHub pull requests" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/graphs/contributors"><img src="https://img.shields.io/github/contributors/mgonzs13/llama_ros.svg" alt="Contributors" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/actions/workflows/python-formatter.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/python-formatter.yml/badge.svg?branch=main" alt="Python Formatter Check" style="pointer-events: none;" class="inline"/></a> <a href="https://github.com/mgonzs13/llama_ros/actions/workflows/cpp-formatter.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/cpp-formatter.yml/badge.svg?branch=main" alt="C++ Formatter Check" style="pointer-events: none;" class="inline"/></a></div><div align="center"><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">ROS 2 Distro   </th><th class="markdownTableHeadCenter">Branch   </th><th class="markdownTableHeadCenter">Build status   </th><th class="markdownTableHeadCenter">Docker Image   </th><th class="markdownTableHeadNone">Documentation    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><b>Humble</b>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/tree/main"><code>main</code></a>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/actions/workflows/humble-docker-build.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/humble-docker-build.yml/badge.svg?branch=main" alt="Humble Build" style="pointer-events: none;" class="inline"/></a>   </td><td class="markdownTableBodyCenter"><a href="https://hub.docker.com/r/mgons/llama_ros/tags?name=humble"><img src="https://img.shields.io/badge/Docker%20Image%20-humble-blue" alt="Docker Image" class="inline"/></a>   </td><td class="markdownTableBodyNone"><a href="https://mgonzs13.github.io/llama_ros/latest"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/doxygen-deployment.yml/badge.svg" alt="Doxygen Deployment" style="pointer-events: none;" class="inline"/></a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><b>Iron</b>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/tree/main"><code>main</code></a>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/actions/workflows/iron-docker-build.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/iron-docker-build.yml/badge.svg?branch=main" alt="Iron Build" style="pointer-events: none;" class="inline"/></a>   </td><td class="markdownTableBodyCenter"><a href="https://hub.docker.com/r/mgons/llama_ros/tags?name=iron"><img src="https://img.shields.io/badge/Docker%20Image%20-iron-blue" alt="Docker Image" class="inline"/></a>   </td><td class="markdownTableBodyNone"><a href="https://mgonzs13.github.io/llama_ros/latest"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/doxygen-deployment.yml/badge.svg" alt="Doxygen Deployment" style="pointer-events: none;" class="inline"/></a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><b>Jazzy</b>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/tree/main"><code>main</code></a>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/actions/workflows/jazzy-docker-build.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/jazzy-docker-build.yml/badge.svg?branch=main" alt="Jazzy Build" style="pointer-events: none;" class="inline"/></a>   </td><td class="markdownTableBodyCenter"><a href="https://hub.docker.com/r/mgons/llama_ros/tags?name=jazzy"><img src="https://img.shields.io/badge/Docker%20Image%20-jazzy-blue" alt="Docker Image" class="inline"/></a>   </td><td class="markdownTableBodyNone"><a href="https://mgonzs13.github.io/llama_ros/latest"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/doxygen-deployment.yml/badge.svg" alt="Doxygen Deployment" style="pointer-events: none;" class="inline"/></a>    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter"><b>Kilted</b>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/tree/main"><code>main</code></a>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/actions/workflows/kilted-docker-build.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/kilted-docker-build.yml/badge.svg?branch=main" alt="Kilted Build" style="pointer-events: none;" class="inline"/></a>   </td><td class="markdownTableBodyCenter"><a href="https://hub.docker.com/r/mgons/llama_ros/tags?name=kilted"><img src="https://img.shields.io/badge/Docker%20Image%20-kilted-blue" alt="Docker Image" class="inline"/></a>   </td><td class="markdownTableBodyNone"><a href="https://mgonzs13.github.io/llama_ros/latest"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/doxygen-deployment.yml/badge.svg" alt="Doxygen Deployment" style="pointer-events: none;" class="inline"/></a>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter"><b>Rolling</b>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/tree/main"><code>main</code></a>   </td><td class="markdownTableBodyCenter"><a href="https://github.com/mgonzs13/llama_ros/actions/workflows/rolling-docker-build.yml?branch=main"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/rolling-docker-build.yml/badge.svg?branch=main" alt="Rolling Build" style="pointer-events: none;" class="inline"/></a>   </td><td class="markdownTableBodyCenter"><a href="https://hub.docker.com/r/mgons/llama_ros/tags?name=rolling"><img src="https://img.shields.io/badge/Docker%20Image%20-rolling-blue" alt="Docker Image" class="inline"/></a>   </td><td class="markdownTableBodyNone"><a href="https://mgonzs13.github.io/llama_ros/latest"><img src="https://github.com/mgonzs13/llama_ros/actions/workflows/doxygen-deployment.yml/badge.svg" alt="Doxygen Deployment" style="pointer-events: none;" class="inline"/></a>   </td></tr>
</table>
</div><div align="center"></div><h1><a class="anchor" id="autotoc_md1"></a>
Table of Contents</h1>
<ol type="1">
<li>Related Projects</li>
<li>Installation</li>
<li>Docker</li>
<li>Usage<ul>
<li><a class="el" href="namespacellama__cli.html">llama_cli</a></li>
<li>Launch Files</li>
<li>LoRA Adapters</li>
<li>ROS 2 Clients</li>
<li>LangChain</li>
</ul>
</li>
<li>Demos</li>
</ol>
<h1><a class="anchor" id="autotoc_md2"></a>
Related Projects</h1>
<ul>
<li><a href="https://github.com/mgonzs13/chatbot_ros">chatbot_ros</a> &rarr; This chatbot, integrated into ROS 2, uses <a href="https://github.com/mgonzs13/whisper_ros/tree/main">whisper_ros</a>, to listen to people speech; and <a class="el" href="namespacellama__ros.html">llama_ros</a>, to generate responses. The chatbot is controlled by a state machine created with <a href="https://github.com/uleroboticsgroup/yasmin">YASMIN</a>.</li>
<li><a href="https://github.com/Dsobh/explainable_ROS">explainable_ros</a> &rarr; A ROS 2 tool to explain the behavior of a robot. Using the integration of LangChain, logs are stored in a vector database. Then, RAG is applied to retrieve relevant logs for user questions answered with <a class="el" href="namespacellama__ros.html">llama_ros</a>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md3"></a>
Installation</h1>
<p>To run <a class="el" href="namespacellama__ros.html">llama_ros</a> with CUDA, first, you must install the <a href="https://developer.nvidia.com/cuda-toolkit">CUDA Toolkit</a>. Then, you can compile <a class="el" href="namespacellama__ros.html">llama_ros</a> with <code>--cmake-args -DGGML_CUDA=ON</code> to enable CUDA support.</p>
<div class="fragment"><div class="line">cd ~/ros2_ws/src</div>
<div class="line">git clone https://github.com/mgonzs13/llama_ros.git</div>
<div class="line">pip3 install -r llama_ros/requirements.txt</div>
<div class="line">cd ~/ros2_ws</div>
<div class="line">rosdep install --from-paths src --ignore-src -r -y</div>
<div class="line">colcon build --cmake-args -DGGML_CUDA=ON # add this for CUDA</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md4"></a>
Docker</h1>
<p>Build the <a class="el" href="namespacellama__ros.html">llama_ros</a> docker or download an image from <a href="https://hub.docker.com/repository/docker/mgons/llama_ros">DockerHub</a>. You can choose to build <a class="el" href="namespacellama__ros.html">llama_ros</a> with CUDA (<code>USE_CUDA</code>) and choose the CUDA version (<code>CUDA_VERSION</code>). Remember that you have to use <code>DOCKER_BUILDKIT=0</code> to compile <a class="el" href="namespacellama__ros.html">llama_ros</a> with CUDA when building the image.</p>
<div class="fragment"><div class="line">DOCKER_BUILDKIT=0 docker build -t llama_ros --build-arg USE_CUDA=1 --build-arg CUDA_VERSION=12-6 .</div>
</div><!-- fragment --><p>Run the docker container. If you want to use CUDA, you have to install the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">NVIDIA Container Tollkit</a> and add <code>--gpus all</code>.</p>
<div class="fragment"><div class="line">docker run -it --rm --gpus all llama_ros</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md5"></a>
Usage</h1>
<h2><a class="anchor" id="autotoc_md6"></a>
llama_cli</h2>
<p>Commands are included in <a class="el" href="namespacellama__ros.html">llama_ros</a> to speed up the test of GGUF-based LLMs within the ROS 2 ecosystem. This way, the following commands are integrating into the ROS 2 commands:</p>
<h3><a class="anchor" id="autotoc_md7"></a>
launch</h3>
<p>Using this command launch a LLM from a YAML file. The configuration of the YAML is used to launch the LLM in the same way as using a regular launch file. Here is an example of how to use it:</p>
<div class="fragment"><div class="line">ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/models/StableLM-Zephyr.yaml</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md8"></a>
prompt</h3>
<p>Using this command send a prompt to a launched LLM. The command uses a string, which is the prompt and has the following arguments:</p>
<ul>
<li>(<code>-r</code>, <code>--reset</code>): Whether to reset the LLM before prompting</li>
<li>(<code>-t</code>, <code>--temp</code>): The temperature value</li>
<li>(<code>--image-url</code>): Image url to sent to a VLM</li>
</ul>
<p>Here is an example of how to use it:</p>
<div class="fragment"><div class="line">ros2 llama prompt &quot;Do you know ROS 2?&quot; -t 0.0</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md9"></a>
Launch Files</h2>
<p>First of all, you need to create a launch file to use <a class="el" href="namespacellama__ros.html">llama_ros</a> or <a class="el" href="namespacellava__ros.html">llava_ros</a>. This launch file will contain the main parameters to download the model from HuggingFace and configure it. Take a look at the following examples and the <a href="llama_bringup/launch">predefined launch files</a>.</p>
<h3><a class="anchor" id="autotoc_md10"></a>
llama_ros (Python Launch)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">from</span> launch <span class="keyword">import</span> LaunchDescription</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__bringup_1_1utils.html">llama_bringup.utils</a> <span class="keyword">import</span> create_llama_launch</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>generate_launch_description():</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> LaunchDescription([</div>
<div class="line">        create_llama_launch(</div>
<div class="line">            n_ctx=2048, <span class="comment"># context of the LLM in tokens</span></div>
<div class="line">            n_batch=8, <span class="comment"># batch size in tokens</span></div>
<div class="line">            n_gpu_layers=0, <span class="comment"># layers to load in GPU</span></div>
<div class="line">            n_threads=1, <span class="comment"># threads</span></div>
<div class="line">            n_predict=2048, <span class="comment"># max tokens, -1 == inf</span></div>
<div class="line"> </div>
<div class="line">            model_repo=<span class="stringliteral">&quot;TheBloke/Marcoroni-7B-v3-GGUF&quot;</span>, <span class="comment"># Hugging Face repo</span></div>
<div class="line">            model_filename=<span class="stringliteral">&quot;marcoroni-7b-v3.Q4_K_M.gguf&quot;</span>, <span class="comment"># model file in repo</span></div>
<div class="line"> </div>
<div class="line">            system_prompt_type=<span class="stringliteral">&quot;Alpaca&quot;</span> <span class="comment"># system prompt type</span></div>
<div class="line">        )</div>
<div class="line">    ])</div>
<div class="ttc" id="anamespacellama__bringup_1_1utils_html"><div class="ttname"><a href="namespacellama__bringup_1_1utils.html">llama_bringup.utils</a></div><div class="ttdef"><b>Definition</b> utils.py:1</div></div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 launch llama_bringup marcoroni.launch.py</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md11"></a>
llama_ros (YAML Config)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line">n_ctx: 2048 # context of the LLM in tokens</div>
<div class="line">n_batch: 8 # batch size in tokens</div>
<div class="line">n_gpu_layers: 0 # layers to load in GPU</div>
<div class="line">n_threads: 1 # threads</div>
<div class="line">n_predict: 2048 # max tokens, -1 == inf</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;cstr/Spaetzle-v60-7b-GGUF&quot; # Hugging Face repo</div>
<div class="line">model_filename: &quot;Spaetzle-v60-7b-q4-k-m.gguf&quot; # model file in repo</div>
<div class="line"> </div>
<div class="line">system_prompt_type: &quot;Alpaca&quot; # system prompt type</div>
</div><!-- fragment --><div class="fragment"><div class="line"><span class="keyword">import</span> os</div>
<div class="line"><span class="keyword">from</span> launch <span class="keyword">import</span> LaunchDescription</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__bringup_1_1utils.html">llama_bringup.utils</a> <span class="keyword">import</span> create_llama_launch_from_yaml</div>
<div class="line"><span class="keyword">from</span> ament_index_python.packages <span class="keyword">import</span> get_package_share_directory</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>generate_launch_description():</div>
<div class="line">    <span class="keywordflow">return</span> LaunchDescription([</div>
<div class="line">        create_llama_launch_from_yaml(os.path.join(</div>
<div class="line">            get_package_share_directory(<span class="stringliteral">&quot;llama_bringup&quot;</span>), <span class="stringliteral">&quot;models&quot;</span>, <span class="stringliteral">&quot;Spaetzle.yaml&quot;</span>))</div>
<div class="line">    ])</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 launch llama_bringup spaetzle.launch.py</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md12"></a>
llama_ros (YAML Config + model shards)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line">n_ctx: 2048 # context of the LLM in tokens</div>
<div class="line">n_batch: 8 # batch size in tokens</div>
<div class="line">n_gpu_layers: 0 # layers to load in GPU</div>
<div class="line">n_threads: 1 # threads</div>
<div class="line">n_predict: 2048 # max tokens, -1 == inf</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&quot; # Hugging Face repo</div>
<div class="line">model_filename: &quot;qwen2.5-coder-7b-instruct-q4_k_m-00001-of-00002.gguf&quot; # model shard file in repo</div>
<div class="line"> </div>
<div class="line">system_prompt_type: &quot;ChatML&quot; # system prompt type</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 llama launch Qwen2.yaml</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md13"></a>
llava_ros (Python Launch)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">from</span> launch <span class="keyword">import</span> LaunchDescription</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__bringup_1_1utils.html">llama_bringup.utils</a> <span class="keyword">import</span> create_llama_launch</div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>generate_launch_description():</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> LaunchDescription([</div>
<div class="line">        create_llama_launch(</div>
<div class="line">            use_llava=<span class="keyword">True</span>, <span class="comment"># enable llava</span></div>
<div class="line"> </div>
<div class="line">            n_ctx=8192, <span class="comment"># context of the LLM in tokens, use a huge context size to load images</span></div>
<div class="line">            n_batch=512, <span class="comment"># batch size in tokens</span></div>
<div class="line">            n_gpu_layers=33, <span class="comment"># layers to load in GPU</span></div>
<div class="line">            n_threads=1, <span class="comment"># threads</span></div>
<div class="line">            n_predict=8192, <span class="comment"># max tokens, -1 == inf</span></div>
<div class="line"> </div>
<div class="line">            model_repo=<span class="stringliteral">&quot;cjpais/llava-1.6-mistral-7b-gguf&quot;</span>, <span class="comment"># Hugging Face repo</span></div>
<div class="line">            model_filename=<span class="stringliteral">&quot;llava-v1.6-mistral-7b.Q4_K_M.gguf&quot;</span>, <span class="comment"># model file in repo</span></div>
<div class="line"> </div>
<div class="line">            mmproj_repo=<span class="stringliteral">&quot;cjpais/llava-1.6-mistral-7b-gguf&quot;</span>, <span class="comment"># Hugging Face repo</span></div>
<div class="line">            mmproj_filename=<span class="stringliteral">&quot;mmproj-model-f16.gguf&quot;</span>, <span class="comment"># mmproj file in repo</span></div>
<div class="line"> </div>
<div class="line">            system_prompt_type=<span class="stringliteral">&quot;Mistral&quot;</span> <span class="comment"># system prompt type</span></div>
<div class="line">        )</div>
<div class="line">    ])</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 launch llama_bringup llava.launch.py</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md14"></a>
llava_ros (YAML Config)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line">use_llava: True # enable llava</div>
<div class="line"> </div>
<div class="line">n_ctx: 8192 # context of the LLM in tokens use a huge context size to load images</div>
<div class="line">n_batch: 512 # batch size in tokens</div>
<div class="line">n_gpu_layers: 33 # layers to load in GPU</div>
<div class="line">n_threads: 1 # threads</div>
<div class="line">n_predict: 8192 # max tokens -1 : :  inf</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;cjpais/llava-1.6-mistral-7b-gguf&quot; # Hugging Face repo</div>
<div class="line">model_filename: &quot;llava-v1.6-mistral-7b.Q4_K_M.gguf&quot; # model file in repo</div>
<div class="line"> </div>
<div class="line">mmproj_repo: &quot;cjpais/llava-1.6-mistral-7b-gguf&quot; # Hugging Face repo</div>
<div class="line">mmproj_filename: &quot;mmproj-model-f16.gguf&quot; # mmproj file in repo</div>
<div class="line"> </div>
<div class="line">system_prompt_type: &quot;mistral&quot; # system prompt type</div>
</div><!-- fragment --><div class="fragment"><div class="line"><span class="keyword">def </span>generate_launch_description():</div>
<div class="line">    <span class="keywordflow">return</span> LaunchDescription([</div>
<div class="line">        create_llama_launch_from_yaml(os.path.join(</div>
<div class="line">            get_package_share_directory(<span class="stringliteral">&quot;llama_bringup&quot;</span>),</div>
<div class="line">            <span class="stringliteral">&quot;models&quot;</span>, <span class="stringliteral">&quot;llava-1.6-mistral-7b-gguf.yaml&quot;</span>))</div>
<div class="line">    ])</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 launch llama_bringup llava.launch.py</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md15"></a>
llava_ros (Audio)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line">use_llava: True</div>
<div class="line"> </div>
<div class="line">n_ctx: 8192</div>
<div class="line">n_batch: 512</div>
<div class="line">n_gpu_layers: 29</div>
<div class="line">n_threads: -1</div>
<div class="line">n_predict: 8192</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;mradermacher/Qwen2-Audio-7B-Instruct-GGUF&quot;</div>
<div class="line">model_filename: &quot;Qwen2-Audio-7B-Instruct.Q4_K_M.gguf&quot;</div>
<div class="line"> </div>
<div class="line">mmproj_repo: &quot;mradermacher/Qwen2-Audio-7B-Instruct-GGUF&quot;</div>
<div class="line">mmproj_filename: &quot;Qwen2-Audio-7B-Instruct.mmproj-f16.gguf&quot;</div>
<div class="line"> </div>
<div class="line">system_prompt_type: &quot;ChatML&quot;</div>
</div><!-- fragment --><div class="fragment"><div class="line"><span class="keyword">def </span>generate_launch_description():</div>
<div class="line">    <span class="keywordflow">return</span> LaunchDescription([</div>
<div class="line">        create_llama_launch_from_yaml(os.path.join(</div>
<div class="line">            get_package_share_directory(<span class="stringliteral">&quot;llama_bringup&quot;</span>),</div>
<div class="line">            <span class="stringliteral">&quot;models&quot;</span>, <span class="stringliteral">&quot;Qwen2-Audio.yaml&quot;</span>))</div>
<div class="line">    ])</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 launch llama_bringup llava.launch.py</div>
</div><!-- fragment --><p></p>
</details>
<h2><a class="anchor" id="autotoc_md16"></a>
LoRA Adapters</h2>
<p>You can use LoRA adapters when launching LLMs. Using <a class="el" href="llama_8cpp.html">llama.cpp</a> features, you can load multiple adapters choosing the scale to apply for each adapter. Here you have an example of using LoRA adapters with Phi-3. You can lis the LoRAs using the <code>/llama/list_loras</code> service and modify their scales values by using the <code>/llama/update_loras</code> service. A scale value of 0.0 means not using that LoRA.</p>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line">n_ctx: 2048</div>
<div class="line">n_batch: 8</div>
<div class="line">n_gpu_layers: 0</div>
<div class="line">n_threads: 1</div>
<div class="line">n_predict: 2048</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;bartowski/Phi-3.5-mini-instruct-GGUF&quot;</div>
<div class="line">model_filename: &quot;Phi-3.5-mini-instruct-Q4_K_M.gguf&quot;</div>
<div class="line"> </div>
<div class="line">lora_adapters:</div>
<div class="line">  - repo: &quot;zhhan/adapter-Phi-3-mini-4k-instruct_code_writing&quot;</div>
<div class="line">    filename: &quot;Phi-3-mini-4k-instruct-adaptor-f16-code_writer.gguf&quot;</div>
<div class="line">    scale: 0.5</div>
<div class="line">  - repo: &quot;zhhan/adapter-Phi-3-mini-4k-instruct_summarization&quot;</div>
<div class="line">    filename: &quot;Phi-3-mini-4k-instruct-adaptor-f16-summarization.gguf&quot;</div>
<div class="line">    scale: 0.5</div>
<div class="line"> </div>
<div class="line">system_prompt_type: &quot;Phi-3&quot;</div>
</div><!-- fragment --><p></p>
</details>
<h2><a class="anchor" id="autotoc_md17"></a>
ROS 2 Clients</h2>
<p>Both <a class="el" href="namespacellama__ros.html">llama_ros</a> and <a class="el" href="namespacellava__ros.html">llava_ros</a> provide ROS 2 interfaces to access the main functionalities of the models. Here you have some examples of how to use them inside ROS 2 nodes. Moreover, take a look to the <a href="llama_demos/llama_demos/llama_demo_node.py">llama_demo_node.py</a> and <a href="llama_demos/llama_demos/llava_demo_node.py">llava_demo_node.py</a> demos.</p>
<h3><a class="anchor" id="autotoc_md18"></a>
Tokenize</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</div>
<div class="line"><span class="keyword">from</span> llama_msgs.srv <span class="keyword">import</span> Tokenize</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExampleNode(Node):</div>
<div class="line">    <span class="keyword">def </span>__init__(self) -&gt; None:</div>
<div class="line">        super().__init__(<span class="stringliteral">&quot;example_node&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the client</span></div>
<div class="line">        self.srv_client = self.create_client(Tokenize, <span class="stringliteral">&quot;/llama/tokenize&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the request</span></div>
<div class="line">        req = Tokenize.Request()</div>
<div class="line">        req.text = <span class="stringliteral">&quot;Example text&quot;</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># call the tokenize service</span></div>
<div class="line">        self.srv_client.wait_for_service()</div>
<div class="line">        tokens = self.srv_client.call(req).tokens</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md19"></a>
Detokenize</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</div>
<div class="line"><span class="keyword">from</span> llama_msgs.srv <span class="keyword">import</span> Detokenize</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExampleNode(Node):</div>
<div class="line">    <span class="keyword">def </span>__init__(self) -&gt; None:</div>
<div class="line">        super().__init__(<span class="stringliteral">&quot;example_node&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the client</span></div>
<div class="line">        self.srv_client = self.create_client(Detokenize, <span class="stringliteral">&quot;/llama/detokenize&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the request</span></div>
<div class="line">        req = Detokenize.Request()</div>
<div class="line">        req.tokens = [123, 123]</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># call the tokenize service</span></div>
<div class="line">        self.srv_client.wait_for_service()</div>
<div class="line">        text = self.srv_client.call(req).text</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md20"></a>
Embeddings</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<p><em>Remember to launch <a class="el" href="namespacellama__ros.html">llama_ros</a> with embedding set to true to be able of generating embeddings with your LLM.</em></p>
<div class="fragment"><div class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</div>
<div class="line"><span class="keyword">from</span> llama_msgs.srv <span class="keyword">import</span> Embeddings</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExampleNode(Node):</div>
<div class="line">    <span class="keyword">def </span>__init__(self) -&gt; None:</div>
<div class="line">        super().__init__(<span class="stringliteral">&quot;example_node&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the client</span></div>
<div class="line">        self.srv_client = self.create_client(Embeddings, <span class="stringliteral">&quot;/llama/generate_embeddings&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the request</span></div>
<div class="line">        req = Embeddings.Request()</div>
<div class="line">        req.prompt = <span class="stringliteral">&quot;Example text&quot;</span></div>
<div class="line">        req.normalize = <span class="keyword">True</span></div>
<div class="line"> </div>
<div class="line">        <span class="comment"># call the embedding service</span></div>
<div class="line">        self.srv_client.wait_for_service()</div>
<div class="line">        embeddings = self.srv_client.call(req).embeddings</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md21"></a>
Generate Response</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</div>
<div class="line"><span class="keyword">from</span> rclpy.action <span class="keyword">import</span> ActionClient</div>
<div class="line"><span class="keyword">from</span> llama_msgs.action <span class="keyword">import</span> GenerateResponse</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExampleNode(Node):</div>
<div class="line">    <span class="keyword">def </span>__init__(self) -&gt; None:</div>
<div class="line">        super().__init__(<span class="stringliteral">&quot;example_node&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the client</span></div>
<div class="line">        self.action_client = ActionClient(</div>
<div class="line">            self, GenerateResponse, <span class="stringliteral">&quot;/llama/generate_response&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the goal and set the sampling config</span></div>
<div class="line">        goal = GenerateResponse.Goal()</div>
<div class="line">        goal.prompt = self.prompt</div>
<div class="line">        goal.sampling_config.temp = 0.2</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait for the server and send the goal</span></div>
<div class="line">        self.action_client.wait_for_server()</div>
<div class="line">        send_goal_future = self.action_client.send_goal_async(</div>
<div class="line">            goal)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait for the server</span></div>
<div class="line">        rclpy.spin_until_future_complete(self, send_goal_future)</div>
<div class="line">        get_result_future = send_goal_future.result().get_result_async()</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait again and take the result</span></div>
<div class="line">        rclpy.spin_until_future_complete(self, get_result_future)</div>
<div class="line">        result: GenerateResponse.Result = get_result_future.result().result</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md22"></a>
Generate Response (llava)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> cv2</div>
<div class="line"><span class="keyword">from</span> cv_bridge <span class="keyword">import</span> CvBridge</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> rclpy.node <span class="keyword">import</span> Node</div>
<div class="line"><span class="keyword">from</span> rclpy.action <span class="keyword">import</span> ActionClient</div>
<div class="line"><span class="keyword">from</span> llama_msgs.action <span class="keyword">import</span> GenerateResponse</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>ExampleNode(Node):</div>
<div class="line">    <span class="keyword">def </span>__init__(self) -&gt; None:</div>
<div class="line">        super().__init__(<span class="stringliteral">&quot;example_node&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create a cv bridge for the image</span></div>
<div class="line">        self.cv_bridge = CvBridge()</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the client</span></div>
<div class="line">        self.action_client = ActionClient(</div>
<div class="line">            self, GenerateResponse, <span class="stringliteral">&quot;/llama/generate_response&quot;</span>)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># create the goal and set the sampling config</span></div>
<div class="line">        goal = GenerateResponse.Goal()</div>
<div class="line">        goal.prompt = self.prompt</div>
<div class="line">        goal.sampling_config.temp = 0.2</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># add your image to the goal</span></div>
<div class="line">        image = cv2.imread(<span class="stringliteral">&quot;/path/to/your/image&quot;</span>, cv2.IMREAD_COLOR)</div>
<div class="line">        goal.images.append(self.cv_bridge.cv2_to_imgmsg(image))</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait for the server and send the goal</span></div>
<div class="line">        self.action_client.wait_for_server()</div>
<div class="line">        send_goal_future = self.action_client.send_goal_async(</div>
<div class="line">            goal)</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait for the server</span></div>
<div class="line">        rclpy.spin_until_future_complete(self, send_goal_future)</div>
<div class="line">        get_result_future = send_goal_future.result().get_result_async()</div>
<div class="line"> </div>
<div class="line">        <span class="comment"># wait again and take the result</span></div>
<div class="line">        rclpy.spin_until_future_complete(self, get_result_future)</div>
<div class="line">        result: GenerateResponse.Result = get_result_future.result().result</div>
</div><!-- fragment --><p></p>
</details>
<h2><a class="anchor" id="autotoc_md23"></a>
LangChain</h2>
<p>There is a <a href="llama_ros/llama_ros/langchain/">llama_ros integration for LangChain</a>. Thus, prompt engineering techniques could be applied. Here you have an example to use it.</p>
<h3><a class="anchor" id="autotoc_md24"></a>
llama_ros (Chain)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROS</div>
<div class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</div>
<div class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the llama_ros llm for langchain</span></div>
<div class="line">llm = LlamaROS()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create a prompt template</span></div>
<div class="line">prompt_template = <span class="stringliteral">&quot;tell me a joke about {topic}&quot;</span></div>
<div class="line">prompt = PromptTemplate(</div>
<div class="line">    input_variables=[<span class="stringliteral">&quot;topic&quot;</span>],</div>
<div class="line">    template=prompt_template</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create a chain with the llm and the prompt template</span></div>
<div class="line">chain = prompt | llm | StrOutputParser()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># run the chain</span></div>
<div class="line">text = chain.invoke({<span class="stringliteral">&quot;topic&quot;</span>: <span class="stringliteral">&quot;bears&quot;</span>})</div>
<div class="line">print(text)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
<div class="ttc" id="anamespacellama__ros_1_1langchain_html"><div class="ttname"><a href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a></div><div class="ttdef"><b>Definition</b> __init__.py:1</div></div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md25"></a>
llama_ros (Stream)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROS</div>
<div class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</div>
<div class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the llama_ros llm for langchain</span></div>
<div class="line">llm = LlamaROS()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create a prompt template</span></div>
<div class="line">prompt_template = <span class="stringliteral">&quot;tell me a joke about {topic}&quot;</span></div>
<div class="line">prompt = PromptTemplate(</div>
<div class="line">    input_variables=[<span class="stringliteral">&quot;topic&quot;</span>],</div>
<div class="line">    template=prompt_template</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create a chain with the llm and the prompt template</span></div>
<div class="line">chain = prompt | llm | StrOutputParser()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># run the chain</span></div>
<div class="line"><span class="keywordflow">for</span> c <span class="keywordflow">in</span> chain.stream({<span class="stringliteral">&quot;topic&quot;</span>: <span class="stringliteral">&quot;bears&quot;</span>}):</div>
<div class="line">    print(c, flush=<span class="keyword">True</span>, end=<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md26"></a>
llava_ros</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROS</div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the llama_ros llm for langchain</span></div>
<div class="line">llm = LlamaROS()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># bind the url_image</span></div>
<div class="line">image_url = <span class="stringliteral">&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&quot;</span></div>
<div class="line">llm = llm.bind(image_url=image_url).stream(<span class="stringliteral">&quot;Describe the image&quot;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># run the llm</span></div>
<div class="line"><span class="keywordflow">for</span> c <span class="keywordflow">in</span> llm:</div>
<div class="line">    print(c, flush=<span class="keyword">True</span>, end=<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md27"></a>
llama_ros_embeddings (RAG)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROSEmbeddings</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the llama_ros embeddings for langchain</span></div>
<div class="line">embeddings = LlamaROSEmbeddings()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create a vector database and assign it</span></div>
<div class="line">db = Chroma(embedding_function=embeddings)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the retriever</span></div>
<div class="line">retriever = db.as_retriever(search_kwargs={<span class="stringliteral">&quot;k&quot;</span>: 5})</div>
<div class="line"> </div>
<div class="line"><span class="comment"># add your texts</span></div>
<div class="line">db.add_texts(texts=[<span class="stringliteral">&quot;your_texts&quot;</span>])</div>
<div class="line"> </div>
<div class="line"><span class="comment"># retrieve documents</span></div>
<div class="line">documents = retriever.invoke(<span class="stringliteral">&quot;your_query&quot;</span>)</div>
<div class="line">print(documents)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md28"></a>
llama_ros (Renranker)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROSReranker</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> LlamaROSEmbeddings</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</div>
<div class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</div>
<div class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</div>
<div class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> ContextualCompressionRetriever</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># load the documents</span></div>
<div class="line">documents = TextLoader(<span class="stringliteral">&quot;../state_of_the_union.txt&quot;</span>,).load()</div>
<div class="line">text_splitter = RecursiveCharacterTextSplitter(</div>
<div class="line">    chunk_size=500, chunk_overlap=100)</div>
<div class="line">texts = text_splitter.split_documents(documents)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the llama_ros embeddings</span></div>
<div class="line">embeddings = LlamaROSEmbeddings()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the VD and the retriever</span></div>
<div class="line">retriever = FAISS.from_documents(</div>
<div class="line">    texts, embeddings).as_retriever(search_kwargs={<span class="stringliteral">&quot;k&quot;</span>: 20})</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the compressor using the llama_ros reranker</span></div>
<div class="line">compressor = LlamaROSReranker()</div>
<div class="line">compression_retriever = ContextualCompressionRetriever(</div>
<div class="line">    base_compressor=compressor, base_retriever=retriever</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># retrieve the documents</span></div>
<div class="line">compressed_docs = compression_retriever.invoke(</div>
<div class="line">    <span class="stringliteral">&quot;What did the president say about Ketanji Jackson Brown&quot;</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">for</span> doc <span class="keywordflow">in</span> compressed_docs:</div>
<div class="line">    print(<span class="stringliteral">&quot;-&quot;</span> * 50)</div>
<div class="line">    print(doc.page_content)</div>
<div class="line">    print(<span class="stringliteral">&quot;\n&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md29"></a>
llama_ros (LLM + RAG + Reranker)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> bs4</div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</div>
<div class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> WebBaseLoader</div>
<div class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</div>
<div class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> SystemMessage</div>
<div class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, HumanMessagePromptTemplate</div>
<div class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</div>
<div class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> ContextualCompressionRetriever</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS, LlamaROSEmbeddings, LlamaROSReranker</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># load, chunk and index the contents of the blog</span></div>
<div class="line">loader = WebBaseLoader(</div>
<div class="line">    web_paths=(<span class="stringliteral">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span>,),</div>
<div class="line">    bs_kwargs=dict(</div>
<div class="line">        parse_only=bs4.SoupStrainer(class_=(<span class="stringliteral">&quot;post-content&quot;</span>, <span class="stringliteral">&quot;post-title&quot;</span>, <span class="stringliteral">&quot;post-header&quot;</span>))</div>
<div class="line">    ),</div>
<div class="line">)</div>
<div class="line">docs = loader.load()</div>
<div class="line"> </div>
<div class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)</div>
<div class="line">splits = text_splitter.split_documents(docs)</div>
<div class="line">vectorstore = Chroma.from_documents(documents=splits, embedding=LlamaROSEmbeddings())</div>
<div class="line"> </div>
<div class="line"><span class="comment"># retrieve and generate using the relevant snippets of the blog</span></div>
<div class="line">retriever = vectorstore.as_retriever(search_kwargs={<span class="stringliteral">&quot;k&quot;</span>: 20})</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create prompt</span></div>
<div class="line">prompt = ChatPromptTemplate.from_messages(</div>
<div class="line">    [</div>
<div class="line">        SystemMessage(<span class="stringliteral">&quot;You are an AI assistant that answer questions briefly.&quot;</span>),</div>
<div class="line">        HumanMessagePromptTemplate.from_template(</div>
<div class="line">            <span class="stringliteral">&quot;Taking into account the followin information:{context}\n\n{question}&quot;</span></div>
<div class="line">        ),</div>
<div class="line">    ]</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create rerank compression retriever</span></div>
<div class="line">compressor = LlamaROSReranker(top_n=3)</div>
<div class="line">compression_retriever = ContextualCompressionRetriever(</div>
<div class="line">    base_compressor=compressor, base_retriever=retriever</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>format_docs(docs):</div>
<div class="line">    formated_docs = <span class="stringliteral">&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">for</span> d <span class="keywordflow">in</span> docs:</div>
<div class="line">        formated_docs += f<span class="stringliteral">&quot;\n\n\t- {d.page_content}&quot;</span></div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> formated_docs</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="comment"># create and use the chain</span></div>
<div class="line">rag_chain = (</div>
<div class="line">    {<span class="stringliteral">&quot;context&quot;</span>: compression_retriever | format_docs, <span class="stringliteral">&quot;question&quot;</span>: RunnablePassthrough()}</div>
<div class="line">    | prompt</div>
<div class="line">    | ChatLlamaROS(temp=0.0)</div>
<div class="line">    | StrOutputParser()</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">for</span> c <span class="keywordflow">in</span> rag_chain.stream(<span class="stringliteral">&quot;What is Task Decomposition?&quot;</span>):</div>
<div class="line">    print(c, flush=<span class="keyword">True</span>, end=<span class="stringliteral">&quot;&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md30"></a>
chat_llama_ros (Chat + VLM)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> SystemMessage</div>
<div class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, HumanMessagePromptTemplate</div>
<div class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create chat</span></div>
<div class="line">chat = ChatLlamaROS(</div>
<div class="line">    temp=0.2,</div>
<div class="line">    penalty_last_n=8</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create prompt template with messages</span></div>
<div class="line">prompt = ChatPromptTemplate.from_messages([</div>
<div class="line">    SystemMessage(<span class="stringliteral">&quot;You are a IA that just answer with a single word.&quot;</span>),</div>
<div class="line">    HumanMessagePromptTemplate.from_template(template=[</div>
<div class="line">        {<span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;text&quot;</span>, <span class="stringliteral">&quot;text&quot;</span>: <span class="stringliteral">&quot;&lt;__media__&gt;Who is the character in the middle of the image?&quot;</span>},</div>
<div class="line">        {<span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;image_url&quot;</span>, <span class="stringliteral">&quot;image_url&quot;</span>: <span class="stringliteral">&quot;{image_url}&quot;</span>}</div>
<div class="line">    ])</div>
<div class="line">])</div>
<div class="line"> </div>
<div class="line"><span class="comment"># create the chain</span></div>
<div class="line">chain = prompt | chat | StrOutputParser()</div>
<div class="line"> </div>
<div class="line"><span class="comment"># stream and print the LLM output</span></div>
<div class="line"><span class="keywordflow">for</span> text <span class="keywordflow">in</span> chain.stream({<span class="stringliteral">&quot;image_url&quot;</span>: <span class="stringliteral">&quot;https://pics.filmaffinity.com/Dragon_Ball_Bola_de_Dragaon_Serie_de_TV-973171538-large.jpg&quot;</span>}):</div>
<div class="line">    print(text, end=<span class="stringliteral">&quot;&quot;</span>, flush=<span class="keyword">True</span>)</div>
<div class="line"> </div>
<div class="line">print(<span class="stringliteral">&quot;&quot;</span>, end=<span class="stringliteral">&quot;\n&quot;</span>, flush=<span class="keyword">True</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md31"></a>
chat_llama_ros (Chat + Audio)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> sys</div>
<div class="line"><span class="keyword">import</span> time</div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> SystemMessage</div>
<div class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, HumanMessagePromptTemplate</div>
<div class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span><a class="code hl_function" href="test__register_8cpp.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a>():</div>
<div class="line">    <span class="keywordflow">if</span> len(sys.argv) &lt; 2:</div>
<div class="line">        prompt = <span class="stringliteral">&quot;What&#39;s that sound?&quot;</span></div>
<div class="line">    <span class="keywordflow">else</span>:</div>
<div class="line">        prompt = <span class="stringliteral">&quot; &quot;</span>.join(sys.argv[1:])</div>
<div class="line"> </div>
<div class="line">    tokens = 0</div>
<div class="line">    initial_time = -1</div>
<div class="line">    eval_time = -1</div>
<div class="line"> </div>
<div class="line">    rclpy.init()</div>
<div class="line">    chat = ChatLlamaROS(temp=0.0)</div>
<div class="line"> </div>
<div class="line">    prompt = ChatPromptTemplate.from_messages(</div>
<div class="line">        [</div>
<div class="line">            SystemMessage(<span class="stringliteral">&quot;You are an IA that answer questions.&quot;</span>),</div>
<div class="line">            HumanMessagePromptTemplate.from_template(</div>
<div class="line">                template=[</div>
<div class="line">                    {<span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;text&quot;</span>, <span class="stringliteral">&quot;text&quot;</span>: f<span class="stringliteral">&quot;&lt;__media__&gt;{prompt}&quot;</span>},</div>
<div class="line">                    {<span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;image_url&quot;</span>, <span class="stringliteral">&quot;image_url&quot;</span>: <span class="stringliteral">&quot;{audio_url}&quot;</span>},</div>
<div class="line">                ]</div>
<div class="line">            ),</div>
<div class="line">        ]</div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">    chain = prompt | chat | StrOutputParser()</div>
<div class="line"> </div>
<div class="line">    initial_time = time.time()</div>
<div class="line">    <span class="keywordflow">for</span> text <span class="keywordflow">in</span> chain.stream(</div>
<div class="line">        {</div>
<div class="line">            <span class="stringliteral">&quot;audio_url&quot;</span>: <span class="stringliteral">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3&quot;</span></div>
<div class="line">        }</div>
<div class="line">    ):</div>
<div class="line">        tokens += 1</div>
<div class="line">        print(text, end=<span class="stringliteral">&quot;&quot;</span>, flush=<span class="keyword">True</span>)</div>
<div class="line">        <span class="keywordflow">if</span> eval_time &lt; 0:</div>
<div class="line">            eval_time = time.time()</div>
<div class="line"> </div>
<div class="line">    print(<span class="stringliteral">&quot;&quot;</span>, end=<span class="stringliteral">&quot;\n&quot;</span>, flush=<span class="keyword">True</span>)</div>
<div class="line"> </div>
<div class="line">    end_time = time.time()</div>
<div class="line">    print(f<span class="stringliteral">&quot;Time to eval: {eval_time - initial_time} s&quot;</span>)</div>
<div class="line">    print(f<span class="stringliteral">&quot;Prediction speed: {tokens / (end_time - eval_time)} t/s&quot;</span>)</div>
<div class="line"> </div>
<div class="line">    rclpy.shutdown()</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line">    <a class="code hl_function" href="test__register_8cpp.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a>()</div>
<div class="ttc" id="atest__register_8cpp_html_a3c04138a5bfe5d72780bb7e82a18e627"><div class="ttname"><a href="test__register_8cpp.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a></div><div class="ttdeci">int main(int argc, char **argv)</div><div class="ttdef"><b>Definition</b> test_register.cpp:46</div></div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md32"></a>
chat_llama_ros (Structured output)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>Joke(BaseModel):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Joke to tell user.&quot;&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line">    setup: str = Field(description=<span class="stringliteral">&quot;The setup of the joke&quot;</span>)</div>
<div class="line">    punchline: str = Field(description=<span class="stringliteral">&quot;The punchline to the joke&quot;</span>)</div>
<div class="line">    rating: Optional[int] = Field(</div>
<div class="line">        default=<span class="keywordtype">None</span>, description=<span class="stringliteral">&quot;How funny the joke is, from 1 to 10&quot;</span></div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">chat = ChatLlamaROS(temp=0.6, penalty_last_n=8)</div>
<div class="line"> </div>
<div class="line">structured_chat = chat.with_structured_output(</div>
<div class="line">    Joke, method=<span class="stringliteral">&quot;function_calling&quot;</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">prompt = ChatPromptTemplate.from_messages(</div>
<div class="line">    [</div>
<div class="line">        HumanMessagePromptTemplate.from_template(</div>
<div class="line">            template=[</div>
<div class="line">                {<span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;text&quot;</span>, <span class="stringliteral">&quot;text&quot;</span>: <span class="stringliteral">&quot;{prompt}&quot;</span>},</div>
<div class="line">            ]</div>
<div class="line">        ),</div>
<div class="line">    ]</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">chain = prompt | structured_chat</div>
<div class="line"> </div>
<div class="line">res = chain.invoke({<span class="stringliteral">&quot;prompt&quot;</span>: <span class="stringliteral">&quot;Tell me a joke about cats&quot;</span>})</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Response: {response.content.strip()}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md33"></a>
chat_llama_ros (Tools)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<p>The current implementation of Tools allows executing tools without requiring a model trained for that task.</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> tool</div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">@tool</span></div>
<div class="line"><span class="keyword">def </span>get_inhabitants(city: str) -&gt; int:</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Get the current temperature of a city&quot;&quot;&quot;</span></div>
<div class="line">    <span class="keywordflow">return</span> randint(4_000_000, 8_000_000)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">@tool</span></div>
<div class="line"><span class="keyword">def </span>get_curr_temperature(city: str) -&gt; int:</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Get the current temperature of a city&quot;&quot;&quot;</span></div>
<div class="line">    <span class="keywordflow">return</span> randint(20, 30)</div>
<div class="line"> </div>
<div class="line">chat = ChatLlamaROS(temp=0.6, penalty_last_n=8)</div>
<div class="line"> </div>
<div class="line">messages = [</div>
<div class="line">    HumanMessage(</div>
<div class="line">        <span class="stringliteral">&quot;What is the current temperature in Madrid? And its inhabitants?&quot;</span></div>
<div class="line">    )</div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line">llm_tools = chat.bind_tools(</div>
<div class="line">    [get_inhabitants, get_curr_temperature], tool_choice=<span class="stringliteral">&#39;any&#39;</span></div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">all_tools_res = llm_tools.invoke(messages)</div>
<div class="line">messages.append(all_tools_res)</div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">for</span> tool <span class="keywordflow">in</span> all_tools_res.tool_calls:</div>
<div class="line">    selected_tool = {</div>
<div class="line">        <span class="stringliteral">&quot;get_inhabitants&quot;</span>: get_inhabitants, <span class="stringliteral">&quot;get_curr_temperature&quot;</span>: get_curr_temperature</div>
<div class="line">    }[tool[<span class="stringliteral">&#39;name&#39;</span>]]</div>
<div class="line"> </div>
<div class="line">    tool_msg = selected_tool.invoke(tool)</div>
<div class="line"> </div>
<div class="line">    formatted_output = f<span class="stringliteral">&quot;{tool[&#39;name&#39;]}({&#39;&#39;.join(tool[&#39;args&#39;].values())}) = {tool_msg.content}&quot;</span></div>
<div class="line"> </div>
<div class="line">    tool_msg.additional_kwargs = {<span class="stringliteral">&#39;args&#39;</span>: tool[<span class="stringliteral">&#39;args&#39;</span>]}</div>
<div class="line">    messages.append(tool_msg)</div>
<div class="line"> </div>
<div class="line">res = llm_tools.invoke(messages)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Response: {res.content}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md34"></a>
chat_llama_ros (Reasoning)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<p>A reasoning model is required, such as Deepseek R1</p>
<div class="fragment"><div class="line"><span class="keyword">import</span> time</div>
<div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line">chat = ChatLlamaROS(temp=0.6, penalty_last_n=8)</div>
<div class="line"> </div>
<div class="line">messages = [</div>
<div class="line">    HumanMessage(</div>
<div class="line">        <span class="stringliteral">&quot;Here we have a book, a laptop, 9 eggs and a nail. Please tell me how to stack them onto each other in a stable manner.&quot;</span></div>
<div class="line">    )</div>
<div class="line">]</div>
<div class="line"> </div>
<div class="line">res = chat.invoke(messages)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Response: {res.content.strip()}&quot;</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;Reasoning: {res.additional_kwargs[&quot;</span>reasoning_content<span class="stringliteral">&quot;]}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h3><a class="anchor" id="autotoc_md35"></a>
chat_llama_ros (LangGraph)</h3>
<details >
<summary >
Click to expand</summary>
<p></p>
<div class="fragment"><div class="line"><span class="keyword">import</span> time</div>
<div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> rclpy</div>
<div class="line"> </div>
<div class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> tool</div>
<div class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</div>
<div class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> create_react_agent</div>
<div class="line"><span class="keyword">from</span> <a class="code hl_namespace" href="namespacellama__ros_1_1langchain.html">llama_ros.langchain</a> <span class="keyword">import</span> ChatLlamaROS</div>
<div class="line"> </div>
<div class="line">rclpy.init()</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">@tool</span></div>
<div class="line"><span class="keyword">def </span>get_inhabitants(city: str) -&gt; int:</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Get the current temperature of a city&quot;&quot;&quot;</span></div>
<div class="line">    <span class="keywordflow">return</span> randint(4_000_000, 8_000_000)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">@tool</span></div>
<div class="line"><span class="keyword">def </span>get_curr_temperature(city: str) -&gt; int:</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;Get the current temperature of a city&quot;&quot;&quot;</span></div>
<div class="line">    <span class="keywordflow">return</span> randint(20, 30)</div>
<div class="line"> </div>
<div class="line">chat = ChatLlamaROS(temp=0.0)</div>
<div class="line"> </div>
<div class="line">agent_executor = create_react_agent(</div>
<div class="line">    self.chat, [get_inhabitants, get_curr_temperature]</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">response = self.agent_executor.invoke(</div>
<div class="line">    {</div>
<div class="line">        <span class="stringliteral">&quot;messages&quot;</span>: [</div>
<div class="line">            HumanMessage(</div>
<div class="line">                content=<span class="stringliteral">&quot;What is the current temperature in Madrid? And its inhabitants?&quot;</span></div>
<div class="line">            )</div>
<div class="line">        ]</div>
<div class="line">    }</div>
<div class="line">)</div>
<div class="line"> </div>
<div class="line">print(f<span class="stringliteral">&quot;Response: {response[&#39;messages&#39;][-1].content}&quot;</span>)</div>
<div class="line"> </div>
<div class="line">rclpy.shutdown()</div>
</div><!-- fragment --><p></p>
</details>
<h1><a class="anchor" id="autotoc_md36"></a>
Demos</h1>
<h2><a class="anchor" id="autotoc_md37"></a>
LLM Demo</h2>
<div class="fragment"><div class="line">ros2 launch llama_bringup spaetzle.launch.py</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos llama_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/mgonzs13/llama_ros/assets/25979134/9311761b-d900-4e58-b9f8-11c8efefdac4">https://github.com/mgonzs13/llama_ros/assets/25979134/9311761b-d900-4e58-b9f8-11c8efefdac4</a></p>
<h2><a class="anchor" id="autotoc_md38"></a>
Embeddings Generation Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/models/bge-base-en-v1.5.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos llama_embeddings_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/7d722017-27dc-417c-ace7-bf6b747e4ced">https://github.com/user-attachments/assets/7d722017-27dc-417c-ace7-bf6b747e4ced</a></p>
<h2><a class="anchor" id="autotoc_md39"></a>
Reranking Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/models/jina-reranker.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos llama_rerank_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/4b4adb4d-7c70-43ea-a2c1-9be57d211484">https://github.com/user-attachments/assets/4b4adb4d-7c70-43ea-a2c1-9be57d211484</a></p>
<h2><a class="anchor" id="autotoc_md40"></a>
VLM Demo</h2>
<div class="fragment"><div class="line">ros2 launch llama_bringup minicpm-2.6.launch.py</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos llava_demo_node --ros-args -p prompt:=&quot;your prompt&quot; -p image_url:=&quot;url of the image&quot; -p use_image:=&quot;whether to send the image&quot;</div>
</div><!-- fragment --><p><a href="https://github.com/mgonzs13/llama_ros/assets/25979134/4a9ef92f-9099-41b4-8350-765336e3503c">https://github.com/mgonzs13/llama_ros/assets/25979134/4a9ef92f-9099-41b4-8350-765336e3503c</a></p>
<h2><a class="anchor" id="autotoc_md41"></a>
Chat Template Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch MiniCPM-2.6.yaml</div>
</div><!-- fragment --><details >
<summary >
Click to expand MiniCPM-2.6.yaml</summary>
<p></p>
<div class="fragment"><div class="line">use_llava: True</div>
<div class="line"> </div>
<div class="line">n_ctx: 8192</div>
<div class="line">n_batch: 512</div>
<div class="line">n_gpu_layers: 20</div>
<div class="line">n_threads: -1</div>
<div class="line">n_predict: 8192</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;openbmb/MiniCPM-V-2_6-gguf&quot;</div>
<div class="line">model_filename: &quot;ggml-model-Q4_K_M.gguf&quot;</div>
<div class="line"> </div>
<div class="line">mmproj_repo: &quot;openbmb/MiniCPM-V-2_6-gguf&quot;</div>
<div class="line">mmproj_filename: &quot;mmproj-model-f16.gguf&quot;</div>
</div><!-- fragment --><p></p>
</details>
<div class="fragment"><div class="line">ros2 run llama_demos chatllama_demo_node</div>
</div><!-- fragment --><p><a href="https://github-production-user-asset-6210df.s3.amazonaws.com/55236157/363094669-c6de124a-4e91-4479-99b6-685fecb0ac20.webm?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240830%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240830T081232Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=f937758f4bcbaec7683e46ddb057fb642dc86a33cc8c736fca3b5ce2bf06ddac&amp;X-Amz-SignedHeaders=host&amp;actor_id=55236157&amp;key_id=0&amp;repo_id=622137360">ChatLlamaROS demo</a></p>
<h2><a class="anchor" id="autotoc_md42"></a>
Chat Structed Output Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch Qwen2.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos chatllama_structured_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/e0bf4031-50c0-4790-94a0-1f6aed5734ec">Structured Output ChatLlama</a></p>
<h2><a class="anchor" id="autotoc_md43"></a>
Chat Tools Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch Qwen2.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos chatllama_tools_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/b912ee29-1466-4d6a-888b-9a2d9c16ae1d">Tools ChatLlama</a></p>
<h2><a class="anchor" id="autotoc_md44"></a>
Chat Reasoning Demo (DeepSeek-R1)</h2>
<div class="fragment"><div class="line">ros2 llama launch DeepSeek-R1.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 run llama_demos chatllama_reasoning_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/3f268614-eabc-4499-b50f-a76d76908d9d">DeepSeekR1 ChatLlama</a></p>
<h2><a class="anchor" id="autotoc_md45"></a>
Langgraph Demo</h2>
<div class="fragment"><div class="line">ros2 llama launch Qwen2.yaml</div>
</div><!-- fragment --><details >
<summary >
Click to expand Qwen2.yaml</summary>
<p></p>
<div class="fragment"><div class="line">_ctx: 4096</div>
<div class="line">n_batch: 256</div>
<div class="line">n_gpu_layers: 29</div>
<div class="line">n_threads: -1</div>
<div class="line">n_predict: -1</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&quot;</div>
<div class="line">model_filename: &quot;qwen2.5-coder-7b-instruct-q4_k_m-00001-of-00002.gguf&quot;</div>
</div><!-- fragment --><p></p>
</details>
<div class="fragment"><div class="line">ros2 run llama_demos chatllama_langgraph_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/a0991cb4-f7f4-43d5-b629-3b1819aead0d">Langgraph ChatLlama</a></p>
<h2><a class="anchor" id="autotoc_md46"></a>
RAG Demo (LLM + chat template + RAG + Reranking + Stream)</h2>
<div class="fragment"><div class="line">ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/models/bge-base-en-v1.5.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 llama launch ~/ros2_ws/src/llama_ros/llama_bringup/models/jina-reranker.yaml</div>
</div><!-- fragment --><div class="fragment"><div class="line">ros2 llama launch Qwen2.yaml</div>
</div><!-- fragment --><details >
<summary >
Click to expand Qwen2.yaml</summary>
<p></p>
<div class="fragment"><div class="line">_ctx: 4096</div>
<div class="line">n_batch: 256</div>
<div class="line">n_gpu_layers: 29</div>
<div class="line">n_threads: -1</div>
<div class="line">n_predict: -1</div>
<div class="line"> </div>
<div class="line">model_repo: &quot;Qwen/Qwen2.5-Coder-3B-Instruct-GGUF&quot;</div>
<div class="line">model_filename: &quot;qwen2.5-coder-3b-instruct-q4_k_m.gguf&quot;</div>
<div class="line"> </div>
<div class="line">stopping_words: [&quot;&lt;|im_end|&gt;&quot;]</div>
</div><!-- fragment --><p></p>
</details>
<div class="fragment"><div class="line">ros2 run llama_demos llama_rag_demo_node</div>
</div><!-- fragment --><p><a href="https://github.com/user-attachments/assets/b4e3957d-1f92-427b-a1a8-cfc76737c0d6">https://github.com/user-attachments/assets/b4e3957d-1f92-427b-a1a8-cfc76737c0d6</a> </p>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2
</small></address>
</div><!-- doc-content -->
</body>
</html>
